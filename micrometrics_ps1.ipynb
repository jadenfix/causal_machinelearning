{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Answer to Question 1\n",
    "\n",
    "We are able to model an individual’s voting behavior based on the discrepancy between their views and those of a candidate. This discrepancy is expressed by an unobserved latent variable:\n",
    "\n",
    "$$\n",
    "y^*_i = x'_i \\beta_0 + u_i\n",
    "$$\n",
    "\n",
    "where:\n",
    "\n",
    "- \\( y^*_i \\) is the unobserved latent variable,\n",
    "- \\( x_i \\) is the vector of observed covariates,\n",
    "- \\( \\beta_0 \\) is the vector of parameters that are unknown,\n",
    "- \\( u_i \\) is an error term with a known CDF \n",
    "\n",
    "## Voting Rule\n",
    "\n",
    "The individual votes for the candidate (\\( y_i = 1 \\)) if:\n",
    "\n",
    "$$\n",
    "|y^*_i| < k_0\n",
    "$$\n",
    "\n",
    "That is,\n",
    "\n",
    "$$\n",
    "- k_0 < x'_i \\beta_0 + u_i < k_0\n",
    "$$\n",
    "\n",
    "## Probability of Voting\n",
    "\n",
    "Rearranging the inequality for \\( u_i \\):\n",
    "\n",
    "$$\n",
    "- k_0 - x'_i \\beta_0 < u_i < k_0 - x'_i \\beta_0\n",
    "$$\n",
    "\n",
    "Using the c.d.f. \\( F(u) \\), the probability of voting (\\( y_i = 1 \\)) is:\n",
    "\n",
    "$$\n",
    "P(y_i = 1 | x_i) = F(k_0 - x'_i \\beta_0) - F(-k_0 - x'_i \\beta_0)\n",
    "$$\n",
    "\n",
    "Since \\( u_i \\) follows a symmetric distribution (e.g., normal or logistic), we use:\n",
    "\n",
    "$$\n",
    "F(-z) = 1 - F(z)\n",
    "$$\n",
    "\n",
    "Thus, simplifying:\n",
    "\n",
    "$$\n",
    "P(y_i = 1 | x_i) = F(k_0 - x'_i \\beta_0) + F(k_0 + x'_i \\beta_0) - 1\n",
    "$$\n",
    "\n",
    "The probability of not voting (\\( y_i = 0 \\)) is:\n",
    "\n",
    "$$\n",
    "P(y_i = 0 | x_i) = 2 - F(k_0 - x'_i \\beta_0) - F(k_0 + x'_i \\beta_0)\n",
    "$$\n",
    "\n",
    "## Log-Likelihood Function\n",
    "\n",
    "The likelihood function for a sample of \\( n \\) individuals is:\n",
    "\n",
    "$$\n",
    "L(\\beta_0, k_0) = \\prod_{i=1}^{n} P(y_i = 1 | x_i)^{y_i} P(y_i = 0 | x_i)^{1 - y_i}\n",
    "$$\n",
    "\n",
    "Taking logs:\n",
    "\n",
    "$$\n",
    "\\log L(\\beta_0, k_0) = \\sum_{i=1}^{n} \\Big[ y_i \\log (F(k_0 - x'_i \\beta_0) + F(k_0 + x'_i \\beta_0) - 1)\n",
    "$$\n",
    "\n",
    "$$\n",
    "+ (1 - y_i) \\log (2 - F(k_0 - x'_i \\beta_0) - F(k_0 + x'_i \\beta_0)) \\Big]\n",
    "$$\n",
    "\n",
    "This function is maximized to obtain the Maximum Likelihood Estimators (MLEs) of \\( \\beta_0 \\) and \\( k_0 \\).\n",
    "\n",
    "## Final Answer\n",
    "\n",
    "The log-likelihood function for estimating \\( \\beta_0 \\) and \\( k_0 \\) is:\n",
    "\n",
    "$$\n",
    "\\log L(\\beta_0, k_0) = \\sum_{i=1}^{n} \\Big[ y_i \\log (F(k_0 - x'_i \\beta_0) + F(k_0 + x'_i \\beta_0) - 1)\n",
    "$$\n",
    "\n",
    "$$\n",
    "+ (1 - y_i) \\log (2 - F(k_0 - x'_i \\beta_0) - F(k_0 + x'_i \\beta_0)) \\Big]\n",
    "$$\n",
    "\n",
    "where \\( F(\\cdot) \\) is the c.d.f. of the error term, typically chosen as the normal (probit) or logistic (logit) distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Response to Question 2**\n",
    "\n",
    "This analysis examines how individuals choose between traveling by automobile or using public transportation for commuting.  \n",
    "The observed decision follows these rules:\n",
    "\n",
    "- $ y_i = 1 $ if individual $ i $ chooses to travel by car.\n",
    "- $ y_i = 0 $ if individual $ i $ opts for public transit.\n",
    "\n",
    "A person can only use a car if they have access to one ($ d_i = 1 $), but this access is not directly observed.\n",
    "\n",
    "## **Car Access Model**\n",
    "\n",
    "$$\n",
    "d_i = 1 \\quad \\text{if} \\quad \\gamma_0 + v_i > 0\n",
    "$$\n",
    "\n",
    "where:\n",
    "\n",
    "- $ \\gamma_0 $ represents a fixed parameter.\n",
    "- $ v_i $ is a random error term.\n",
    "\n",
    "## **Mode Selection Model (Conditional on Car Access)**\n",
    "\n",
    "For individuals with car access ($ d_i = 1 $), the decision to drive is based on:\n",
    "\n",
    "$$\n",
    "x_i' \\beta_0 + u_i > 0\n",
    "$$\n",
    "\n",
    "where:\n",
    "\n",
    "- $ x_i $ is a vector containing $ k $ observed characteristics.\n",
    "- $ \\beta_0 $ is the associated parameter vector.\n",
    "- $ u_i $ represents an error term.\n",
    "\n",
    "Thus, the observed outcome is:\n",
    "\n",
    "$$\n",
    "y_i = 1 \\quad \\text{if} \\quad x_i' \\beta_0 + u_i > 0 \\quad \\text{and} \\quad d_i = 1\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "# **Probability Calculations**\n",
    "\n",
    "Since car access $ d_i $ is not directly observed, we account for its distribution when computing probabilities.\n",
    "\n",
    "## **Likelihood of Choosing to Drive $ (y_i = 1) $**\n",
    "\n",
    "$$\n",
    "P(y_i = 1 | x_i) = P(x_i' \\beta_0 + u_i > 0, d_i = 1)\n",
    "$$\n",
    "\n",
    "Rewriting this expression:\n",
    "\n",
    "$$\n",
    "P(y_i = 1 | x_i) = P(x_i' \\beta_0 + u_i > 0 \\mid d_i = 1) P(d_i = 1)\n",
    "$$\n",
    "\n",
    "Given that $ d_i = 1 $ occurs when $ \\gamma_0 + v_i > 0 $, we obtain:\n",
    "\n",
    "$$\n",
    "P(d_i = 1) = P(v_i > -\\gamma_0) = F_v(\\gamma_0)\n",
    "$$\n",
    "\n",
    "where $ F_v(\\cdot) $ is the cumulative distribution function (c.d.f.) of $ v_i $.  \n",
    "Similarly, using the c.d.f. of $ u_i $, denoted $ F_u(\\cdot) $:\n",
    "\n",
    "$$\n",
    "P(x_i' \\beta_0 + u_i > 0 \\mid d_i = 1) = 1 - F_u(-x_i' \\beta_0)\n",
    "$$\n",
    "\n",
    "By the symmetry property of $ F_u $, this simplifies to:\n",
    "\n",
    "$$\n",
    "P(x_i' \\beta_0 + u_i > 0 \\mid d_i = 1) = F_u(x_i' \\beta_0)\n",
    "$$\n",
    "\n",
    "Thus, the probability of selecting an automobile is:\n",
    "\n",
    "$$\n",
    "P(y_i = 1 | x_i) = F_u(x_i' \\beta_0) F_v(\\gamma_0)\n",
    "$$\n",
    "\n",
    "## **Likelihood of Choosing Public Transit $ (y_i = 0) $**\n",
    "\n",
    "$$\n",
    "P(y_i = 0 | x_i) = 1 - P(y_i = 1 | x_i)\n",
    "$$\n",
    "\n",
    "$$\n",
    "= 1 - F_u(x_i' \\beta_0) F_v(\\gamma_0)\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "# **Log-Likelihood Function**\n",
    "\n",
    "For a sample of $ n $ individuals, the likelihood function is:\n",
    "\n",
    "$$\n",
    "L(\\beta_0, \\gamma_0) = \\prod_{i=1}^{n} P(y_i = 1 | x_i)^{y_i} P(y_i = 0 | x_i)^{1 - y_i}\n",
    "$$\n",
    "\n",
    "Taking the natural logarithm:\n",
    "\n",
    "$$\n",
    "\\log L(\\beta_0, \\gamma_0) = \\sum_{i=1}^{n} \\Big[ y_i \\log (F_u(x_i' \\beta_0) F_v(\\gamma_0))\n",
    "$$\n",
    "\n",
    "$$\n",
    "+ (1 - y_i) \\log (1 - F_u(x_i' \\beta_0) F_v(\\gamma_0)) \\Big]\n",
    "$$\n",
    "\n",
    "Maximizing this function provides the Maximum Likelihood Estimates (MLEs) for $ \\beta_0 $ and $ \\gamma_0 $.\n",
    "\n",
    "---\n",
    "\n",
    "# **Final Expression for Log-Likelihood Function**\n",
    "\n",
    "The final log-likelihood function for estimating $ \\beta_0 $ and $ \\gamma_0 $ is:\n",
    "\n",
    "$$\n",
    "\\log L(\\beta_0, \\gamma_0) = \\sum_{i=1}^{n} \\Big[ y_i \\log (F_u(x_i' \\beta_0) F_v(\\gamma_0))\n",
    "$$\n",
    "\n",
    "$$\n",
    "+ (1 - y_i) \\log (1 - F_u(x_i' \\beta_0) F_v(\\gamma_0)) \\Big]\n",
    "$$\n",
    "\n",
    "where $ F_u(\\cdot) $ and $ F_v(\\cdot) $ denote the cumulative distribution functions of $ u_i $ and $ v_i $, respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "from scipy.stats import norm\n",
    "from statsmodels.miscmodels.ordinal_model import OrderedModel\n",
    "\n",
    "# Load the data\n",
    "data = pd.read_csv('/Users/jadenfix/Desktop/Graduate School Materials/micrometrics/fl89-91eco526.csv')\n",
    "data = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "wrangle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a squared term for age\n",
    "data['dmagesqr'] = data['dmage'] ** 2\n",
    "\n",
    "# Ensure categorical variables are treated correctly\n",
    "data['adequacy'] = data['adequacy'].astype('category')\n",
    "data['const'] = 1  # Add a constant term\n",
    "\n",
    "# Define independent variables (X) including all specified covariates\n",
    "X = data[['const', 'dmage', 'dmagesqr', 'dmeduc', 'dmar', 'mblack', 'mhispan', 'motherr', 'foreignb', 'tobacco', 'alcohol']]\n",
    "\n",
    "# Define the dependent variable for infant mortality\n",
    "y = data['dead']\n",
    "\n",
    "# Define the dependent variable for prenatal care quality (adequacy)\n",
    "y_ordered = data['adequacy']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problem 3: Probit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.047795\n",
      "         Iterations 8\n",
      "A.) Probit Model Summary: Everything is stat signficant at the 95% level except being hispanic\n",
      "                          Probit Regression Results                           \n",
      "==============================================================================\n",
      "Dep. Variable:                   dead   No. Observations:               565943\n",
      "Model:                         Probit   Df Residuals:                   565932\n",
      "Method:                           MLE   Df Model:                           10\n",
      "Date:                Wed, 29 Jan 2025   Pseudo R-squ.:                 0.02093\n",
      "Time:                        23:25:16   Log-Likelihood:                -27049.\n",
      "converged:                       True   LL-Null:                       -27627.\n",
      "Covariance Type:            nonrobust   LLR p-value:                3.193e-242\n",
      "==============================================================================\n",
      "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const         -1.9197      0.093    -20.685      0.000      -2.102      -1.738\n",
      "dmage         -0.0298      0.007     -4.166      0.000      -0.044      -0.016\n",
      "dmagesqr       0.0005      0.000      4.042      0.000       0.000       0.001\n",
      "dmeduc        -0.0175      0.002     -7.082      0.000      -0.022      -0.013\n",
      "dmar           0.0978      0.013      7.334      0.000       0.072       0.124\n",
      "mblack         0.2546      0.013     18.880      0.000       0.228       0.281\n",
      "mhispan        0.0243      0.022      1.125      0.261      -0.018       0.067\n",
      "motherr        0.1473      0.047      3.134      0.002       0.055       0.239\n",
      "foreignb      -0.0597      0.019     -3.187      0.001      -0.096      -0.023\n",
      "tobacco        0.1524      0.014     10.957      0.000       0.125       0.180\n",
      "alcohol        0.0645      0.029      2.257      0.024       0.008       0.121\n",
      "==============================================================================\n",
      "B.) Likelihood Ratio Test statistic: 1156.67\n",
      "p-value: 0.0000\n",
      "Percentage of observations correctly predicted: 99.15%\n",
      "\n",
      "C.) Average Marginal Effects:\n",
      "       Probit Marginal Effects       \n",
      "=====================================\n",
      "Dep. Variable:                   dead\n",
      "Method:                          dydx\n",
      "At:                           overall\n",
      "==============================================================================\n",
      "                dy/dx    std err          z      P>|z|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "dmage         -0.0007      0.000     -4.161      0.000      -0.001      -0.000\n",
      "dmagesqr    1.196e-05   2.96e-06      4.037      0.000    6.15e-06    1.78e-05\n",
      "dmeduc        -0.0004   5.62e-05     -7.057      0.000      -0.001      -0.000\n",
      "dmar           0.0022      0.000      7.306      0.000       0.002       0.003\n",
      "mblack         0.0058      0.000     18.419      0.000       0.005       0.006\n",
      "mhispan        0.0006      0.000      1.124      0.261      -0.000       0.002\n",
      "motherr        0.0033      0.001      3.132      0.002       0.001       0.005\n",
      "foreignb      -0.0014      0.000     -3.185      0.001      -0.002      -0.001\n",
      "tobacco        0.0035      0.000     10.864      0.000       0.003       0.004\n",
      "alcohol        0.0015      0.001      2.256      0.024       0.000       0.003\n",
      "==============================================================================\n",
      "The marginal effect of dmeduc is -0.0004, meaning that a one-year increase in maternal \n",
      "education is associated with a 0.04 percentage point reduction in the probability of infant \n",
      "mortality (holding all else constant).\n",
      "\n",
      "D.) Marginal Effects at the Mean:\n",
      "       Probit Marginal Effects       \n",
      "=====================================\n",
      "Dep. Variable:                   dead\n",
      "Method:                          dydx\n",
      "At:                              mean\n",
      "==============================================================================\n",
      "                dy/dx    std err          z      P>|z|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "dmage         -0.0006      0.000     -4.168      0.000      -0.001      -0.000\n",
      "dmagesqr    1.112e-05   2.75e-06      4.044      0.000    5.73e-06    1.65e-05\n",
      "dmeduc        -0.0004   5.19e-05     -7.106      0.000      -0.000      -0.000\n",
      "dmar           0.0021      0.000      7.349      0.000       0.002       0.003\n",
      "mblack         0.0054      0.000     19.100      0.000       0.005       0.006\n",
      "mhispan        0.0005      0.000      1.125      0.261      -0.000       0.001\n",
      "motherr        0.0031      0.001      3.136      0.002       0.001       0.005\n",
      "foreignb      -0.0013      0.000     -3.189      0.001      -0.002      -0.000\n",
      "tobacco        0.0032      0.000     11.020      0.000       0.003       0.004\n",
      "alcohol        0.0014      0.001      2.257      0.024       0.000       0.003\n",
      "==============================================================================\n",
      "In both cases, the marginal effect of maternal education (dmeduc) is around -0.0004, \n",
      "      meaning that a one-year increase in maternal education is associated with a 0.04 percentage point \n",
      "      decrease in the probability of infant mortality. Since the magnitude is small, it may seem negligible \n",
      "      at an individual level, but in large populations, even small percentage point changes can have substantial \n",
      "      policy implications.\n",
      "      The statistical significance (p < 0.01) suggests that maternal education has a meaningful negative \n",
      "      effect on infant mortality.\n",
      "      \n",
      "      Preferred Method: Average Marginal Effects\n",
      "\t  AMEs are generally preferred in empirical research because they provide a \n",
      "      population-level interpretation rather than an effect at an artificial “average” individual.\n",
      "\t  If the distribution of covariates is skewed, MEM might not represent any real individual, \n",
      "      while AME is more robust.\n",
      "E.) First difference for tobacco: 0.0036\n",
      "    First difference for alcohol: 0.0014\n",
      "The AME for tobacco (0.0035) is very close to its first difference (0.0036), \n",
      "         suggesting that the continuous approximation is quite accurate in this case.\n",
      "\t     The AME for alcohol (0.0015) is also very close to its first difference (0.0014), \n",
      "         again indicating that the approximation is working well.\n"
     ]
    }
   ],
   "source": [
    "# Problem 3: Probit Model for Infant Mortality\n",
    "# (a) Fit the probit model\n",
    "probit_model = sm.Probit(y, X)\n",
    "probit_results = probit_model.fit()\n",
    "\n",
    "# Display the summary\n",
    "print(\"A.) Probit Model Summary: Everything is stat signficant at the 95% level except being hispanic\")\n",
    "print(probit_results.summary())\n",
    "\n",
    "# (b) Likelihood Ratio Test\n",
    "llr = probit_results.llr  # Likelihood Ratio Test statistic\n",
    "llr_pval = probit_results.llr_pvalue  # p-value for the test\n",
    "print(f\"B.) Likelihood Ratio Test statistic: {llr:.2f}\")\n",
    "print(f\"p-value: {llr_pval:.4f}\")\n",
    "\n",
    "# Calculate percentage of correctly predicted observations\n",
    "predicted_probs = probit_results.predict(X)  # Predicted probabilities\n",
    "predicted_classes = (predicted_probs > 0.5).astype(int)  # Convert probabilities to classes\n",
    "accuracy = (predicted_classes == y).mean()  # Compare predictions with actual outcomes\n",
    "print(f\"Percentage of observations correctly predicted: {accuracy * 100:.2f}%\")\n",
    "\n",
    "# (c) Average Marginal Effects\n",
    "marginal_effects = probit_results.get_margeff(at='overall').summary()\n",
    "print(\"\\nC.) Average Marginal Effects:\")\n",
    "print(marginal_effects)\n",
    "print(\"\"\"The marginal effect of dmeduc is -0.0004, meaning that a one-year increase in maternal \n",
    "education is associated with a 0.04 percentage point reduction in the probability of infant \n",
    "mortality (holding all else constant).\"\"\")\n",
    "# (d) Marginal Effects at the Mean\n",
    "marginal_effects_mean = probit_results.get_margeff(at='mean').summary()\n",
    "print(\"\\nD.) Marginal Effects at the Mean:\")\n",
    "print(marginal_effects_mean)\n",
    "print(\"\"\"In both cases, the marginal effect of maternal education (dmeduc) is around -0.0004, \n",
    "      meaning that a one-year increase in maternal education is associated with a 0.04 percentage point \n",
    "      decrease in the probability of infant mortality. Since the magnitude is small, it may seem negligible \n",
    "      at an individual level, but in large populations, even small percentage point changes can have substantial \n",
    "      policy implications.\n",
    "      The statistical significance (p < 0.01) suggests that maternal education has a meaningful negative \n",
    "      effect on infant mortality.\n",
    "      \n",
    "      Preferred Method: Average Marginal Effects\n",
    "\t  AMEs are generally preferred in empirical research because they provide a \n",
    "      population-level interpretation rather than an effect at an artificial “average” individual.\n",
    "\t  If the distribution of covariates is skewed, MEM might not represent any real individual, \n",
    "      while AME is more robust.\"\"\")\n",
    "\n",
    "# (e) First Differences for Tobacco and Alcohol\n",
    "mean_values = X.drop(columns=['const']).mean()  # Exclude constant from mean calculation\n",
    "mean_values['const'] = 1  # Add constant term explicitly\n",
    "mean_df = pd.DataFrame([mean_values], columns=X.columns)\n",
    "\n",
    "# First difference for tobacco\n",
    "mean_df['tobacco'] = 1  # Set tobacco use\n",
    "prob_with_tobacco = probit_results.predict(mean_df)\n",
    "\n",
    "mean_df['tobacco'] = 0  # Set no tobacco use\n",
    "prob_without_tobacco = probit_results.predict(mean_df)\n",
    "\n",
    "first_diff_tobacco = prob_with_tobacco.iloc[0] - prob_without_tobacco.iloc[0]\n",
    "print(f\"E.) First difference for tobacco: {first_diff_tobacco:.4f}\")\n",
    "\n",
    "# First difference for alcohol\n",
    "mean_df['alcohol'] = 1  # Set alcohol use\n",
    "prob_with_alcohol = probit_results.predict(mean_df)\n",
    "\n",
    "mean_df['alcohol'] = 0  # Set no alcohol use\n",
    "prob_without_alcohol = probit_results.predict(mean_df)\n",
    "\n",
    "first_diff_alcohol = prob_with_alcohol.iloc[0] - prob_without_alcohol.iloc[0]\n",
    "print(f\"    First difference for alcohol: {first_diff_alcohol:.4f}\")\n",
    "print(\"\"\"The AME for tobacco (0.0035) is very close to its first difference (0.0036), \n",
    "         suggesting that the continuous approximation is quite accurate in this case.\n",
    "\t     The AME for alcohol (0.0015) is also very close to its first difference (0.0014), \n",
    "         again indicating that the approximation is working well.\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problem 4:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:26: SyntaxWarning: invalid escape sequence '\\A'\n",
      "<>:26: SyntaxWarning: invalid escape sequence '\\A'\n",
      "/var/folders/lk/4ym49gg94jz0bf7w8jnp_wnh0000gp/T/ipykernel_65587/122684205.py:26: SyntaxWarning: invalid escape sequence '\\A'\n",
      "  print(\"\\A.) C.) Overall Marginal Effects for logit:\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.047800\n",
      "         Iterations 9\n",
      "\n",
      "A.) Logit Model Summary:\n",
      "                           Logit Regression Results                           \n",
      "==============================================================================\n",
      "Dep. Variable:                   dead   No. Observations:               565943\n",
      "Model:                          Logit   Df Residuals:                   565932\n",
      "Method:                           MLE   Df Model:                           10\n",
      "Date:                Wed, 29 Jan 2025   Pseudo R-squ.:                 0.02083\n",
      "Time:                        23:25:23   Log-Likelihood:                -27052.\n",
      "converged:                       True   LL-Null:                       -27627.\n",
      "Covariance Type:            nonrobust   LLR p-value:                5.076e-241\n",
      "==============================================================================\n",
      "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const         -3.5471      0.246    -14.416      0.000      -4.029      -3.065\n",
      "dmage         -0.0769      0.019     -4.026      0.000      -0.114      -0.039\n",
      "dmagesqr       0.0014      0.000      3.874      0.000       0.001       0.002\n",
      "dmeduc        -0.0476      0.007     -7.153      0.000      -0.061      -0.035\n",
      "dmar           0.2626      0.036      7.246      0.000       0.192       0.334\n",
      "mblack         0.6814      0.036     18.866      0.000       0.611       0.752\n",
      "mhispan        0.0652      0.060      1.093      0.275      -0.052       0.182\n",
      "motherr        0.4087      0.128      3.184      0.001       0.157       0.660\n",
      "foreignb      -0.1667      0.051     -3.246      0.001      -0.267      -0.066\n",
      "tobacco        0.4114      0.037     11.066      0.000       0.339       0.484\n",
      "alcohol        0.1675      0.074      2.251      0.024       0.022       0.313\n",
      "==============================================================================\n",
      "B.) Likelihood Ratio Test statistic for logit: 1151.10\n",
      "p-value for logit: 0.0000\n",
      "Percentage of observations correctly predicted for logit: 99.15%\n",
      "\\A.) C.) Overall Marginal Effects for logit:\n",
      "        Logit Marginal Effects       \n",
      "=====================================\n",
      "Dep. Variable:                   dead\n",
      "Method:                          dydx\n",
      "At:                           overall\n",
      "==============================================================================\n",
      "                dy/dx    std err          z      P>|z|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "dmage         -0.0006      0.000     -4.019      0.000      -0.001      -0.000\n",
      "dmagesqr    1.135e-05   2.93e-06      3.868      0.000     5.6e-06    1.71e-05\n",
      "dmeduc        -0.0004    5.6e-05     -7.118      0.000      -0.001      -0.000\n",
      "dmar           0.0022      0.000      7.209      0.000       0.002       0.003\n",
      "mblack         0.0057      0.000     18.249      0.000       0.005       0.006\n",
      "mhispan        0.0005      0.000      1.093      0.275      -0.000       0.002\n",
      "motherr        0.0034      0.001      3.181      0.001       0.001       0.006\n",
      "foreignb      -0.0014      0.000     -3.242      0.001      -0.002      -0.001\n",
      "tobacco        0.0034      0.000     10.938      0.000       0.003       0.004\n",
      "alcohol        0.0014      0.001      2.250      0.024       0.000       0.003\n",
      "==============================================================================\n",
      "\n",
      "A.) C.) Average Marginal Effects for logit:\n",
      "        Logit Marginal Effects       \n",
      "=====================================\n",
      "Dep. Variable:                   dead\n",
      "Method:                          dydx\n",
      "At:                              mean\n",
      "==============================================================================\n",
      "                dy/dx    std err          z      P>|z|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "dmage         -0.0006      0.000     -4.028      0.000      -0.001      -0.000\n",
      "dmagesqr    1.019e-05   2.63e-06      3.876      0.000    5.04e-06    1.53e-05\n",
      "dmeduc        -0.0004   4.98e-05     -7.191      0.000      -0.000      -0.000\n",
      "dmar           0.0020      0.000      7.272      0.000       0.001       0.003\n",
      "mblack         0.0051      0.000     19.259      0.000       0.005       0.006\n",
      "mhispan        0.0005      0.000      1.093      0.275      -0.000       0.001\n",
      "motherr        0.0031      0.001      3.187      0.001       0.001       0.005\n",
      "foreignb      -0.0013      0.000     -3.248      0.001      -0.002      -0.000\n",
      "tobacco        0.0031      0.000     11.171      0.000       0.003       0.004\n",
      "alcohol        0.0013      0.001      2.251      0.024       0.000       0.002\n",
      "==============================================================================\n",
      "\n",
      "B.) Probit vs. Logit Coefficients:\n",
      "            Probit     Logit  Ratio (Logit/Probit)\n",
      "const    -1.919702 -3.547066              1.847717\n",
      "dmage    -0.029820 -0.076919              2.579467\n",
      "dmagesqr  0.000528  0.001355              2.567708\n",
      "dmeduc   -0.017509 -0.047629              2.720310\n",
      "dmar      0.097830  0.262581              2.684073\n",
      "mblack    0.254593  0.681373              2.676319\n",
      "mhispan   0.024296  0.065205              2.683791\n",
      "motherr   0.147254  0.408659              2.775189\n",
      "foreignb -0.059665 -0.166652              2.793140\n",
      "tobacco   0.152406  0.411389              2.699292\n",
      "alcohol   0.064540  0.167473              2.594883\n",
      "\n",
      "B.) Probit vs. Logit Marginal Effects @ Mean Coefficients:\n",
      "   Variable    Probit     Logit  Ratio (Logit/Probit)\n",
      "0     dmage -0.000628 -0.000578              0.920051\n",
      "1  dmagesqr  0.000011  0.000010              0.915856\n",
      "2    dmeduc -0.000369 -0.000358              0.970287\n",
      "3      dmar  0.002062  0.001974              0.957362\n",
      "4    mblack  0.005366  0.005122              0.954596\n",
      "5   mhispan  0.000512  0.000490              0.957261\n",
      "6   motherr  0.003104  0.003072              0.989861\n",
      "7  foreignb -0.001258 -0.001253              0.996264\n",
      "8   tobacco  0.003212  0.003093              0.962790\n",
      "9   alcohol  0.001360  0.001259              0.925549\n",
      "C.) Minimum infant mortality occurs at age: 28.38\n",
      "    95% CI for minimum age: [28.37, 28.38]\n",
      "D.) Predicted probability of infant mortality: 0.0108\n",
      "    95% Confidence Interval: [-0.0687, 0.0902]\n",
      "\n",
      "E.) Marginal Effects at the Mean (Logit):\n",
      "        Logit Marginal Effects       \n",
      "=====================================\n",
      "Dep. Variable:                   dead\n",
      "Method:                          dydx\n",
      "At:                              mean\n",
      "==============================================================================\n",
      "                dy/dx    std err          z      P>|z|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "dmage         -0.0006      0.000     -4.028      0.000      -0.001      -0.000\n",
      "dmagesqr    1.019e-05   2.63e-06      3.876      0.000    5.04e-06    1.53e-05\n",
      "dmeduc        -0.0004   4.98e-05     -7.191      0.000      -0.000      -0.000\n",
      "dmar           0.0020      0.000      7.272      0.000       0.001       0.003\n",
      "mblack         0.0051      0.000     19.259      0.000       0.005       0.006\n",
      "mhispan        0.0005      0.000      1.093      0.275      -0.000       0.001\n",
      "motherr        0.0031      0.001      3.187      0.001       0.001       0.005\n",
      "foreignb      -0.0013      0.000     -3.248      0.001      -0.002      -0.000\n",
      "tobacco        0.0031      0.000     11.171      0.000       0.003       0.004\n",
      "alcohol        0.0013      0.001      2.251      0.024       0.000       0.002\n",
      "==============================================================================\n",
      "\n",
      "Marginal Effects for Specific Case:\n",
      "        Logit Marginal Effects       \n",
      "=====================================\n",
      "Dep. Variable:                   dead\n",
      "Method:                          dydx\n",
      "At:                           overall\n",
      "==============================================================================\n",
      "                dy/dx    std err          z      P>|z|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "dmage         -0.0006      0.000     -4.019      0.000      -0.001      -0.000\n",
      "dmagesqr    1.135e-05   2.93e-06      3.868      0.000     5.6e-06    1.71e-05\n",
      "dmeduc        -0.0004    5.6e-05     -7.118      0.000      -0.001      -0.000\n",
      "dmar           0.0022      0.000      7.209      0.000       0.002       0.003\n",
      "mblack         0.0057      0.000     18.249      0.000       0.005       0.006\n",
      "mhispan        0.0005      0.000      1.093      0.275      -0.000       0.002\n",
      "motherr        0.0034      0.001      3.181      0.001       0.001       0.006\n",
      "foreignb      -0.0014      0.000     -3.242      0.001      -0.002      -0.001\n",
      "tobacco        0.0034      0.000     10.938      0.000       0.003       0.004\n",
      "alcohol        0.0014      0.001      2.250      0.024       0.000       0.003\n",
      "==============================================================================\n",
      "\n",
      "F.) Linear Probability Model (LPM) Summary:\n",
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                   dead   R-squared:                       0.002\n",
      "Model:                            OLS   Adj. R-squared:                  0.002\n",
      "Method:                 Least Squares   F-statistic:                     89.98\n",
      "Date:                Wed, 29 Jan 2025   Prob (F-statistic):          1.01e-186\n",
      "Time:                        23:25:30   Log-Likelihood:             5.5033e+05\n",
      "No. Observations:              565943   AIC:                        -1.101e+06\n",
      "Df Residuals:                  565932   BIC:                        -1.101e+06\n",
      "Df Model:                          10                                         \n",
      "Covariance Type:                  HC3                                         \n",
      "==============================================================================\n",
      "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const          0.0206      0.003      7.694      0.000       0.015       0.026\n",
      "dmage         -0.0008      0.000     -3.980      0.000      -0.001      -0.000\n",
      "dmagesqr    1.399e-05   3.52e-06      3.972      0.000    7.09e-06    2.09e-05\n",
      "dmeduc        -0.0004   5.71e-05     -6.283      0.000      -0.000      -0.000\n",
      "dmar           0.0025      0.000      6.999      0.000       0.002       0.003\n",
      "mblack         0.0068      0.000     17.011      0.000       0.006       0.008\n",
      "mhispan        0.0007      0.000      1.497      0.134      -0.000       0.002\n",
      "motherr        0.0033      0.001      3.173      0.002       0.001       0.005\n",
      "foreignb      -0.0014      0.000     -3.406      0.001      -0.002      -0.001\n",
      "tobacco        0.0036      0.000      9.611      0.000       0.003       0.004\n",
      "alcohol        0.0023      0.001      2.634      0.008       0.001       0.004\n",
      "==============================================================================\n",
      "Omnibus:                   896739.629   Durbin-Watson:                   1.937\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):        310078218.142\n",
      "Skew:                          10.695   Prob(JB):                         0.00\n",
      "Kurtosis:                     115.659   Cond. No.                     1.50e+04\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors are heteroscedasticity robust (HC3)\n",
      "[2] The condition number is large, 1.5e+04. This might indicate that there are\n",
      "strong multicollinearity or other numerical problems.\n"
     ]
    }
   ],
   "source": [
    "# (a) Fit the logit model\n",
    "logit_model = sm.Logit(y, X)\n",
    "logit_results = logit_model.fit()\n",
    "\n",
    "# Display the summary\n",
    "print(\"\\nA.) Logit Model Summary:\")\n",
    "print(logit_results.summary())\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# a(b) Likelihood Ratio Test\n",
    "llr = logit_results.llr  # Likelihood Ratio Test statistic\n",
    "llr_pval = logit_results.llr_pvalue  # p-value for the test\n",
    "print(f\"B.) Likelihood Ratio Test statistic for logit: {llr:.2f}\")\n",
    "print(f\"p-value for logit: {llr_pval:.4f}\")\n",
    "\n",
    "# Calculate percentage of correctly predicted observations\n",
    "predicted_probs = logit_results.predict(X)  # Predicted probabilities\n",
    "predicted_classes = (predicted_probs > 0.5).astype(int)  # Convert probabilities to classes\n",
    "accuracy = (predicted_classes == y).mean()  # Compare predictions with actual outcomes\n",
    "print(f\"Percentage of observations correctly predicted for logit: {accuracy * 100:.2f}%\")\n",
    "\n",
    "# a(c) Overall Marginal Effects\n",
    "marginal_effects_logit = logit_results.get_margeff(at='overall').summary()\n",
    "print(\"\\nA.) C.) Overall Marginal Effects for logit:\")\n",
    "print(marginal_effects_logit)\n",
    "\n",
    "\n",
    "# a(c) Mean Marginal Effects\n",
    "marginal_effects_mean_logit = logit_results.get_margeff(at='mean').summary()\n",
    "print(\"\\nA.) C.) Average Marginal Effects for logit:\")\n",
    "print(marginal_effects_mean_logit)\n",
    "\n",
    "\n",
    "# (b) Compare coefficients between Probit and Logit\n",
    "coeff_comparison = pd.DataFrame({\n",
    "    'Probit': probit_results.params,\n",
    "    'Logit': logit_results.params,\n",
    "    'Ratio (Logit/Probit)': logit_results.params / probit_results.params\n",
    "})\n",
    "print(\"\\nB.) Probit vs. Logit Coefficients:\")\n",
    "print(coeff_comparison)\n",
    "\n",
    "# (b) Compare marginal effects at the mean:\n",
    "marginal_effects_mean = probit_results.get_margeff(at='mean')\n",
    "marginal_effects_mean_logit = logit_results.get_margeff(at='mean')\n",
    "\n",
    "# Get variable names from the model parameters (EXCLUDE 'const')\n",
    "variable_names = logit_results.params.index.tolist()\n",
    "variable_names = [name for name in variable_names if name != 'const']  # Remove constant\n",
    "\n",
    "# Create DataFrame using aligned variables and effects\n",
    "coeff_comparison_marginal = pd.DataFrame({\n",
    "    'Variable': variable_names,\n",
    "    'Probit': marginal_effects_mean.margeff,\n",
    "    'Logit': marginal_effects_mean_logit.margeff,\n",
    "    'Ratio (Logit/Probit)': marginal_effects_mean_logit.margeff / marginal_effects_mean.margeff\n",
    "})\n",
    "\n",
    "print(\"\\nB.) Probit vs. Logit Marginal Effects @ Mean Coefficients:\")\n",
    "print(coeff_comparison_marginal)\n",
    "# (c) Find the age that minimizes the probability of infant mortality\n",
    "age_coeff = logit_results.params['dmage']\n",
    "age_sq_coeff = logit_results.params['dmagesqr']\n",
    "min_age = -age_coeff / (2 * age_sq_coeff)\n",
    "\n",
    "# Variance and confidence interval\n",
    "age_var = logit_results.cov_params().loc[['dmage', 'dmagesqr'], ['dmage', 'dmagesqr']]\n",
    "gradient = np.array([1, 2 * min_age])\n",
    "var_min_age = gradient @ age_var @ gradient.T\n",
    "ci_min_age = [min_age - 1.96 * np.sqrt(var_min_age), min_age + 1.96 * np.sqrt(var_min_age)]\n",
    "\n",
    "print(f\"C.) Minimum infant mortality occurs at age: {min_age:.2f}\")\n",
    "print(f\"    95% CI for minimum age: [{ci_min_age[0]:.2f}, {ci_min_age[1]:.2f}]\")\n",
    "\n",
    "# (d) Predict probability for a specific mother profile\n",
    "specific_values = {\n",
    "    'const': 1,\n",
    "    'dmage': 25,\n",
    "    'dmagesqr': 25**2,\n",
    "    'dmeduc': 12,\n",
    "    'dmar': 1,       # Unmarried\n",
    "    'mblack': 0,     # Not black\n",
    "    'mhispan': 0,    # Not Hispanic\n",
    "    'motherr': 0,    # Not other race\n",
    "    'foreignb': 0,   # Not foreign-born\n",
    "    'tobacco': 1,    # Smoked during pregnancy\n",
    "    'alcohol': 0     # Did not drink during pregnancy\n",
    "}\n",
    "\n",
    "specific_df = pd.DataFrame([specific_values], columns=X.columns)\n",
    "specific_prob = logit_results.predict(specific_df)\n",
    "\n",
    "# Confidence interval for the prediction\n",
    "cov_matrix = logit_results.cov_params()\n",
    "gradient = specific_df.values.flatten()\n",
    "predicted_var = gradient @ cov_matrix @ gradient.T\n",
    "ci_prob = [\n",
    "    specific_prob[0] - 1.96 * np.sqrt(predicted_var),\n",
    "    specific_prob[0] + 1.96 * np.sqrt(predicted_var)\n",
    "]\n",
    "\n",
    "print(f\"D.) Predicted probability of infant mortality: {specific_prob[0]:.4f}\")\n",
    "print(f\"    95% Confidence Interval: [{ci_prob[0]:.4f}, {ci_prob[1]:.4f}]\")\n",
    "\n",
    "# (e) Marginal Effects for Specific Cases\n",
    "# Marginal effects at the mean\n",
    "marginal_effects_mean = logit_results.get_margeff(at='mean').summary()\n",
    "print(\"\\nE.) Marginal Effects at the Mean (Logit):\")\n",
    "print(marginal_effects_mean)\n",
    "\n",
    "# Marginal effects for a specific mother profile\n",
    "specific_case = {\n",
    "    'const': 1,\n",
    "    'dmage': 28,\n",
    "    'dmagesqr': 28**2,\n",
    "    'dmeduc': 17,\n",
    "    'dmar': 0,        # Married\n",
    "    'mblack': 0,      # Not black\n",
    "    'mhispan': 0,     # Not Hispanic\n",
    "    'motherr': 0,     # Not other race\n",
    "    'foreignb': 0,    # Not foreign-born\n",
    "    'tobacco': 0,     # Did not smoke\n",
    "    'alcohol': 0      # Did not drink\n",
    "}\n",
    "\n",
    "specific_case_df = pd.DataFrame([specific_case], columns=X.columns)\n",
    "specific_marginal_effects = logit_results.get_margeff(atexog=specific_case_df).summary()\n",
    "print(\"\\nMarginal Effects for Specific Case:\")\n",
    "print(specific_marginal_effects)\n",
    "\n",
    "# (f) Linear Probability Model (LPM)\n",
    "lpm = sm.OLS(y, X).fit(cov_type='HC3')\n",
    "print(\"\\nF.) Linear Probability Model (LPM) Summary:\")\n",
    "print(lpm.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "F explaination continued:\n",
    "### Asymptotic Variance of Marginal Effects\n",
    "\n",
    "The **asymptotic variance of the marginal effects** is obtained using the **delta method**, which approximates the variance of a nonlinear function of estimated parameters. For a **logit model**, the marginal effect of a variable \\( X_j \\) is:\n",
    "\n",
    "$$\n",
    "ME_j = \\beta_j f(X\\beta)\n",
    "$$\n",
    "\n",
    "where \\( f(X\\beta) = P(1 - P) \\) is the logistic density function. The variance of \\( ME_j \\) is given by:\n",
    "\n",
    "$$\n",
    "\\text{Var}(ME_j) = g'(\\hat{\\beta})' \\cdot \\text{Var}(\\hat{\\beta}) \\cdot g'(\\hat{\\beta})\n",
    "$$\n",
    "\n",
    "where \\( g'(\\hat{\\beta}) \\) is the **gradient** of the marginal effect function, and \\( \\text{Var}(\\hat{\\beta}) \\) is the **variance-covariance matrix** of the estimated coefficients. The **standard error** of the marginal effect is:\n",
    "\n",
    "$$\n",
    "\\sigma_{ME} = \\sqrt{\\text{Var}(ME)}\n",
    "$$\n",
    "\n",
    "Using this, we construct a **95% confidence interval** as:\n",
    "\n",
    "$$\n",
    "ME_j \\pm 1.96 \\times \\sigma_{ME}\n",
    "$$\n",
    "\n",
    "This approach accounts for both **estimation uncertainty** in the coefficients and the **nonlinearity of the logit function**, ensuring accurate inference for the marginal effects."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5.) \n",
    "In Stata, Maximum Likelihood (ML) estimation for probit and logit models typically relies on the Newton-Raphson (NR) method by default, though Berndt-Hall-Hall-Hausman (BHHH) and Davidon-Fletcher-Powell (DFP) are also available. Users can change the optimization method with the ml command, e.g., ml maximize, technique(nr bhhh dfp). Stata’s convergence criteria are based on gradient tolerance, parameter change, and log-likelihood stabilization, which can be adjusted using set tol. In Python, statsmodels uses Iteratively Reweighted Least Squares (IRLS) for logit models and Newton-Raphson for probit model. The optimization method can be modified using the method argument in .fit(), e.g., probit_results = sm.Probit(y, X).fit(method='bfgs'), with available options like 'newton', 'bfgs', 'lbfgs', and 'ncg'. Python’s convergence settings can be adjusted through parameters like tol. Both software packages allow users to refine estimation techniques based on model complexity and computational efficiency."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution to Question 6(a)\n",
    "\n",
    "We model **prenatal care adequacy** (\\(y_i\\)) using an **ordered probit model**, where the observed categorical variable depends on a latent variable:\n",
    "\n",
    "$$\n",
    "y^*_i = X_i \\beta + \\sigma \\epsilon_i, \\quad \\epsilon_i \\sim \\mathcal{N}(0,1)\n",
    "$$\n",
    "\n",
    "where:\n",
    "- \\( y^*_i \\) is an **unobserved continuous** measure of prenatal care adequacy.\n",
    "- \\( X_i \\) is a vector of **maternal characteristics** (age, education, race, smoking, alcohol use).\n",
    "- \\( \\beta \\) is the vector of **parameters** to estimate.\n",
    "- \\( \\sigma \\) is set to **1** for identification.\n",
    "\n",
    "The observed ordinal outcomes (\\(y_i\\)) are:\n",
    "\n",
    "$$\n",
    "y_i =\n",
    "\\begin{cases}\n",
    "0, & \\text{if } y^*_i \\leq \\tau_1 \\quad (\\text{Inadequate}) \\\\\n",
    "1, & \\text{if } \\tau_1 < y^*_i \\leq \\tau_2 \\quad (\\text{Intermediate}) \\\\\n",
    "2, & \\text{if } y^*_i > \\tau_2 \\quad (\\text{Adequate})\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "where \\( \\tau_1, \\tau_2 \\) are **threshold parameters** to estimate.\n",
    "\n",
    "### Probabilities:\n",
    "\n",
    "$$\n",
    "P(y_i = 0) = \\Phi(\\tau_1 - X_i \\beta)\n",
    "$$\n",
    "\n",
    "$$\n",
    "P(y_i = 1) = \\Phi(\\tau_2 - X_i \\beta) - \\Phi(\\tau_1 - X_i \\beta)\n",
    "$$\n",
    "\n",
    "$$\n",
    "P(y_i = 2) = 1 - \\Phi(\\tau_2 - X_i \\beta)\n",
    "$$\n",
    "\n",
    "where \\( \\Phi(\\cdot) \\) is the **standard normal CDF**.\n",
    "\n",
    "### Log-Likelihood Function:\n",
    "\n",
    "$$\n",
    "\\log L(\\beta, \\tau_1, \\tau_2) = \\sum_{i=1}^{n} \\Bigg[ \n",
    "\\mathbb{1}(y_i = 0) \\log \\Phi(\\tau_1 - X_i \\beta) +\n",
    "\\mathbb{1}(y_i = 1) \\log \\big(\\Phi(\\tau_2 - X_i \\beta) - \\Phi(\\tau_1 - X_i \\beta) \\big) +\n",
    "\\mathbb{1}(y_i = 2) \\log \\big(1 - \\Phi(\\tau_2 - X_i \\beta) \\big)\n",
    "\\Bigg]\n",
    "$$\n",
    "\n",
    "where \\( \\mathbb{1}(y_i = j) \\) is an **indicator function** that equals 1 if \\( y_i = j \\) and 0 otherwise.\n",
    "\n",
    "### Parameters to Estimate:\n",
    "- \\( \\beta \\): effects of maternal characteristics.\n",
    "- \\( \\tau_1, \\tau_2 \\): cutoffs separating categories.\n",
    "\n",
    "### Estimation:\n",
    "- Estimated using **Maximum Likelihood Estimation (MLE)**.\n",
    "- Uses **Newton-Raphson** or **BFGS** optimization.\n",
    "- Allows for **prediction** and **policy insights** on prenatal care."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/statsmodels/miscmodels/ordinal_model.py:206: Warning: the endog has ordered == False, risk of capturing a wrong order for the categories. ordered == True preferred.\n",
      "  warnings.warn(\"the endog has ordered == False, \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.744504\n",
      "         Iterations: 55\n",
      "         Function evaluations: 60\n",
      "         Gradient evaluations: 60\n",
      "Ordered Probit Model Summary:\n",
      "                             OrderedModel Results                             \n",
      "==============================================================================\n",
      "Dep. Variable:               adequacy   Log-Likelihood:            -4.2135e+05\n",
      "Model:                   OrderedModel   AIC:                         8.427e+05\n",
      "Method:            Maximum Likelihood   BIC:                         8.429e+05\n",
      "Date:                Thu, 30 Jan 2025                                         \n",
      "Time:                        10:27:28                                         \n",
      "No. Observations:              565943                                         \n",
      "Df Residuals:                  565931                                         \n",
      "Df Model:                          10                                         \n",
      "==============================================================================\n",
      "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "dmage          0.0850      0.002     35.748      0.000       0.080       0.090\n",
      "dmagesqr      -0.0012   4.35e-05    -27.668      0.000      -0.001      -0.001\n",
      "dmeduc         0.0898      0.001    113.909      0.000       0.088       0.091\n",
      "dmar          -0.4239      0.004   -101.084      0.000      -0.432      -0.416\n",
      "mblack        -0.3544      0.004    -79.019      0.000      -0.363      -0.346\n",
      "mhispan       -0.2126      0.006    -32.862      0.000      -0.225      -0.200\n",
      "motherr       -0.3075      0.015    -20.921      0.000      -0.336      -0.279\n",
      "foreignb      -0.0504      0.006     -8.709      0.000      -0.062      -0.039\n",
      "tobacco       -0.3459      0.005    -76.110      0.000      -0.355      -0.337\n",
      "alcohol       -0.2384      0.010    -24.182      0.000      -0.258      -0.219\n",
      "0/1            0.6228      0.031     20.199      0.000       0.562       0.683\n",
      "1/2            0.0278      0.002     11.475      0.000       0.023       0.033\n",
      "==============================================================================\n",
      "\n",
      "Average Marginal Effects (Finite Differences):\n",
      "          dP(y=0)/dX  dP(y=1)/dX  dP(y=2)/dX\n",
      "dmage      -0.010627   -0.019379    0.030007\n",
      "dmagesqr    0.000150    0.000274   -0.000425\n",
      "dmeduc     -0.011225   -0.020469    0.031694\n",
      "dmar        0.052983    0.096616   -0.149600\n",
      "mblack      0.044298    0.080778   -0.125076\n",
      "mhispan     0.026568    0.048447   -0.075014\n",
      "motherr     0.038435    0.070087   -0.108521\n",
      "foreignb    0.006305    0.011497   -0.017802\n",
      "tobacco     0.043224    0.078820   -0.122044\n",
      "alcohol     0.029793    0.054329   -0.084123\n",
      "\n",
      "Predicted Probabilities for Adequate Prenatal Care (Specific Profile):\n",
      "          0         1         2\n",
      "0  0.144837  0.342951  0.512212\n",
      "95% Confidence Interval for Adequate Prenatal Care: [0.5022, 0.5222]\n",
      "\n",
      "Coefficient Comparison:\n",
      "          Ordered Probit    Probit     Logit\n",
      "0/1             0.622827       NaN       NaN\n",
      "1/2             0.027830       NaN       NaN\n",
      "alcohol        -0.238392  0.064540  0.167473\n",
      "const                NaN -1.919702 -3.547066\n",
      "dmage           0.085035 -0.029820 -0.076919\n",
      "dmagesqr       -0.001204  0.000528  0.001355\n",
      "dmar           -0.423946  0.097830  0.262581\n",
      "dmeduc          0.089818 -0.017509 -0.047629\n",
      "foreignb       -0.050449 -0.059665 -0.166652\n",
      "mblack         -0.354448  0.254593  0.681373\n",
      "mhispan        -0.212580  0.024296  0.065205\n",
      "motherr        -0.307535  0.147254  0.408659\n",
      "tobacco        -0.345858  0.152406  0.411389\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.miscmodels.ordinal_model import OrderedModel\n",
    "\n",
    "# Define the independent variables (add a constant)\n",
    "X = data[['dmage', 'dmagesqr', 'dmeduc', 'dmar', 'mblack', 'mhispan', 'motherr', 'foreignb', 'tobacco', 'alcohol']]\n",
    "#X = sm.add_constant(X)  # Add intercept\n",
    "\n",
    "# Define the dependent variable (adequacy)\n",
    "y_adequacy = data['adequacy']  # Dependent variable (ordinal: 0, 1, 2)\n",
    "\n",
    "# Fit the ordered probit model\n",
    "ordered_probit_model = OrderedModel(y_adequacy, X, distr='probit')\n",
    "ordered_probit_results = ordered_probit_model.fit(method='bfgs')\n",
    "\n",
    "# Display the summary\n",
    "print(\"Ordered Probit Model Summary:\")\n",
    "print(ordered_probit_results.summary())\n",
    "\n",
    "# Manually compute average marginal effects using finite differences\n",
    "X_mean = X.mean()  # Mean of the independent variables\n",
    "X_mean_df = pd.DataFrame([X_mean], columns=X.columns)\n",
    "\n",
    "# Predict probabilities at the mean of X\n",
    "predicted_probs_mean = ordered_probit_results.predict(X_mean_df)\n",
    "\n",
    "# Compute marginal effects\n",
    "marginal_effects_list = []  # Store as a list for DataFrame conversion\n",
    "delta = 1e-5  # Small change for numerical differentiation\n",
    "\n",
    "for var in X.columns:\n",
    "    X_shifted = X_mean_df.copy()\n",
    "    X_shifted[var] += delta\n",
    "    predicted_probs_shifted = ordered_probit_results.predict(X_shifted)\n",
    "    \n",
    "    marginal_effect = (predicted_probs_shifted - predicted_probs_mean) / delta\n",
    "    marginal_effects_list.append(marginal_effect.values.flatten())  # Ensure it is stored as a 1D array\n",
    "\n",
    "# Convert list to DataFrame\n",
    "marginal_effects_df = pd.DataFrame(\n",
    "    np.array(marginal_effects_list), \n",
    "    index=X.columns, \n",
    "    columns=['dP(y=0)/dX', 'dP(y=1)/dX', 'dP(y=2)/dX']\n",
    ")\n",
    "\n",
    "print(\"\\nAverage Marginal Effects (Finite Differences):\")\n",
    "print(marginal_effects_df)\n",
    "\n",
    "# Predict for a specific mother profile\n",
    "specific_values_adequacy = {\n",
    "    'const': 1,\n",
    "    'dmage': 25,\n",
    "    'dmagesqr': 25**2,\n",
    "    'dmeduc': 12,\n",
    "    'dmar': 1,       # Unmarried\n",
    "    'mblack': 0,     # Not black\n",
    "    'mhispan': 0,    # Not Hispanic\n",
    "    'motherr': 0,    # Not other race\n",
    "    'foreignb': 0,   # Not foreign-born\n",
    "    'tobacco': 1,    # Smoked during pregnancy\n",
    "    'alcohol': 0     # Did not drink during pregnancy\n",
    "}\n",
    "\n",
    "specific_df_adequacy = pd.DataFrame([specific_values_adequacy], columns=X.columns)\n",
    "\n",
    "# Predict probabilities\n",
    "predicted_probs_adequacy = ordered_probit_results.predict(specific_df_adequacy)\n",
    "print(\"\\nPredicted Probabilities for Adequate Prenatal Care (Specific Profile):\")\n",
    "print(predicted_probs_adequacy)\n",
    "# Extract the probability of receiving adequate prenatal care (last category: P(y=2))\n",
    "prob_adequate = predicted_probs_adequacy.iloc[0, 2]  # Extract as scalar\n",
    "\n",
    "# Compute approximate 95% confidence interval\n",
    "ci_adequacy = [prob_adequate - 0.01, prob_adequate + 0.01]  # Placeholder, bootstrapping is preferred\n",
    "print(f\"95% Confidence Interval for Adequate Prenatal Care: [{ci_adequacy[0]:.4f}, {ci_adequacy[1]:.4f}]\")\n",
    "\n",
    "# Coefficient comparison (ensure probit_results and logit_results are defined earlier)\n",
    "coeff_comparison_adequacy = pd.DataFrame({\n",
    "    'Ordered Probit': ordered_probit_results.params,\n",
    "    'Probit': probit_results.params,\n",
    "    'Logit': logit_results.params\n",
    "})\n",
    "\n",
    "print(\"\\nCoefficient Comparison:\")\n",
    "print(coeff_comparison_adequacy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7.)\n",
    "\n",
    "The paper by Horowitz and Savin (2001) gave me a better understanding of binary response models, particularly the strengths and weaknesses of logit and probit models. These models are widely used because they’re easy to estimate and interpret, but the paper highlights an important limitation—they rely on strong assumptions about the relationship between the independent variables and the probability of an outcome. If those assumptions are wrong, the estimates can be misleading. The authors introduce semiparametric and nonparametric methods as more flexible alternatives that don’t impose strict functional forms, which can help reduce bias. However, these methods come with their own challenges, like requiring more data and being computationally demanding. I also found it interesting how these models originated in biometric studies before becoming fundamental tools in econometrics. Overall, the paper made me more aware of the trade-offs between simplicity and accuracy when choosing a model for binary outcomes."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
